# ETF Data Pipeline Toy Project

### **1. 프로젝트 설명 및 목적**

- 이 프로젝트는 데이터 엔지니어링 개념을 연습하기 위한 토이 프로젝트입니다. FastAPI와 MySQL을 사용하여 ETF(상장지수펀드) 데이터를 수집, 저장, 예측하는 데이터 파이프라인을 구축하는 것을 목표로 합니다. 주요 목표는 일일 데이터 업데이트 시스템을 구현하고, Prophet 머신러닝 모델을 사용해 ETF 가격을 예측하는 것입니다.

### **2. 개선 사항**

- 프로젝트를 진행하며 **Docker**, **Redshift**, **Airflow**를 학습하고, 기존의 코드를 개선하여 데이터 파이프라인을 자동화하고 관리하는 서비스를 배포하는 프로젝트로 확장할 예정입니다.
- 특히, **Docker**로 애플리케이션을 컨테이너화하여 일관된 개발 환경과 배포 환경을 구축하고, **Airflow**를 사용해 데이터 수집과 처리를 자동화하며, **Redshift**와 같은 데이터 웨어하우스를 사용하여 대규모 데이터 처리를 지원할 계획입니다.

### **3. 성과**


### **4. 현재의 기술 스택**

- **FastAPI**: 백엔드 API 서버로 사용.
- **MySQL**: 관계형 데이터베이스로 ETF 데이터를 저장.
- **SQLAlchemy**: 데이터베이스와 상호작용을 위한 ORM(Object Relational Mapping).
- **Prophet**: 시계열 데이터 예측을 위한 머신러닝 모델.

기술 스택의 사용 이유:

- Prophet: 간단하게 시계열 데이터를 예측할 수 있고 , 현재 프로젝트의 목적이  etf 예측의 정확성이나 최적화가 아니라, 데이터 파이프라인 구성 및 관리를 목적으로 하기 때문에 최대한 간단한 모델을 사용했습니다.

### **5. 앞으로 추가될 스택 및 구조**

- LSTM : 하이퍼 파라미터를 최적화 하고, 모델의 예측 성능을 더 높일 계획
- **Docker**: 애플리케이션을 컨테이너화하여 일관된 환경에서 배포.
- **Airflow**: 데이터 파이프라인을 자동화하여 일일 데이터 수집 및 처리를 스케줄링.
- **Redshift**: 데이터 웨어하우스 솔루션으로, 대규모 데이터 분석 및 처리 지원.
- **Grafana 또는 Tableau**: 시각화를 통해 예측 데이터를 대시보드로 제공할 계획.


### 느낀 점
`##### 컨테이너 간 의존성 문제 `
- 문제점: Docker Compose를 통해 FastAPI, MySQL, Apache Scheduler 등을 통합 운영하면서 컨테이너 간 의존성 문제로 인해 실행 순서가 맞지 않거나 데이터 타입 불일치로 오류가 발생했습니다. 각 컨테이너는 독립적으로는 정상 작동했으나, 통합 시 의존성 문제로 인해 어려움을 겪었습니다.
- 해결: Docker Compose 파일의 의존성 설정을 조정하고, 서비스 간 실행 순서를 명확히 지정하여 문제를 해결하였습니다. 이를 통해 컨테이너 기반 환경에서의 의존성 관리와 서비스 연동의 중요성을 깨닫게 되었으며, 이 경험을 통해 컨테이너화된 환경에서 각 컨터네이너 간 의존성을 잘 고려해야 안정적인 서비스를 제공할 수 있음을 알게되었습니다.

`##### Nginx를 활용한 배포 및 가상 머신 운영 경험`

- 어려움: Nginx를 통해 애플리케이션을 배포하면서 네트워크 설정과 가상 머신 환경 구성이 처음이라 많은 시행착오를 겪었습니다. 특히, 외부 접속을 위한 네트워크 설정과 애플리케이션 안정성 유지에 어려움이 있었습니다.
- 해결: Nginx 설정을 최적화하고 Azure 가상 머신에서의 네트워크 구성을 조정하여 안정적인 서비스를 제공할 수 있었습니다. 이를 통해 클라우드 환경에서의 애플리케이션 배포와 네트워크 설정의 중요성을 인식하게 되었으며, 다양한 환경에서 일관되게 작동하는 서비스 구조의 필요성을 깨달았습니다.

`##### 데이터 전처리 및 자동화 문제 해결`

- 문제점: Apache Scheduler를 사용해 데이터 전처리와 업데이트를 자동화하는 과정에서 작업 순서와 데이터 타입 불일치로 인해 데이터가 올바르게 처리되지 않는 문제가 발생했습니다. 스케줄링 시 작업 순서와 형식이 맞지 않아 데이터 파이프라인의 신뢰성에 문제가 생겼습니다.
- 해결: 작업 순서를 재조정하고 데이터 타입을 명확히 설정하여 자동화된 파이프라인이 정상적으로 작동하도록 하였습니다. 이를 통해 데이터 파이프라인의 실행 순서 관리와 데이터 일관성 유지의 중요성을 깨닫게 되었으며, 복잡한 워크플로우 자동화를 위해 Airflow와 Spark 도입의 필요성을 절감하였습니다.

`##### 기술 스택 확장 계획`
- 다양한 기술 스택의 연동과 관리에 어려움을 느끼며, 더 복잡하고 대규모 데이터 처리가 가능한 기술 스택이 필요하다고 판단했습니다. 데이터 자동화와 처리 속도 개선을 위해 ETL 기술 도입이 필요함을 느꼈습니다.
Apache Airflow와 Spark, AWS 등 고급 기술 스택을 추가하여 데이터 파이프라인의 유연성과 확장성을 높이기로 계획했습니다. 이를 통해 데이터 자동 업데이트, 대규모 데이터 처리, 안정적인 서비스 운영이 가능하도록 프로젝트를 개선하고, 실무에서 발생할 다양한 데이터 엔지니어링 문제에 대비할 역량을 강화하고자 합니다.
